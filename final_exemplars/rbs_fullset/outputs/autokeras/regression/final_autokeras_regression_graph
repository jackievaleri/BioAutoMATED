digraph {
	node [shape=box]
	0 [label="(4, 17)"]
	1 [label="(4, 64)"]
	2 [label="(4, 64)"]
	3 [label="(4, 64)"]
	4 [label="(4, 64)"]
	5 [label="(4, 64)"]
	6 [label="(4, 64)"]
	7 [label="(4, 64)"]
	8 [label="(4, 64)"]
	9 [label="(4, 64)"]
	10 [label="(4, 64)"]
	11 [label="(4, 64)"]
	12 [label="(4, 64)"]
	13 [label="(4, 64)"]
	14 [label="(4, 64)"]
	15 [label="(4, 64)"]
	16 [label="(4, 64)"]
	17 [label="(4, 64)"]
	18 [label="(4, 64)"]
	19 [label="(4, 64)"]
	20 [label="(4, 64)"]
	21 [label="(4, 64)"]
	22 [label="(4, 64)"]
	23 [label="(2, 128)"]
	24 [label="(2, 128)"]
	25 [label="(2, 128)"]
	26 [label="(2, 128)"]
	27 [label="(4, 64)"]
	28 [label="(2, 128)"]
	29 [label="(2, 128)"]
	30 [label="(2, 128)"]
	31 [label="(2, 128)"]
	32 [label="(2, 128)"]
	33 [label="(2, 128)"]
	34 [label="(2, 128)"]
	35 [label="(2, 128)"]
	36 [label="(2, 128)"]
	37 [label="(2, 128)"]
	38 [label="(2, 128)"]
	39 [label="(2, 128)"]
	40 [label="(2, 128)"]
	41 [label="(1, 256)"]
	42 [label="(1, 256)"]
	43 [label="(1, 256)"]
	44 [label="(1, 256)"]
	45 [label="(2, 128)"]
	46 [label="(1, 256)"]
	47 [label="(1, 256)"]
	48 [label="(1, 256)"]
	49 [label="(1, 256)"]
	50 [label="(1, 256)"]
	51 [label="(1, 256)"]
	52 [label="(1, 256)"]
	53 [label="(1, 256)"]
	54 [label="(1, 256)"]
	55 [label="(1, 256)"]
	56 [label="(1, 256)"]
	57 [label="(1, 256)"]
	58 [label="(1, 256)"]
	59 [label="(1, 512)"]
	60 [label="(1, 512)"]
	61 [label="(1, 512)"]
	62 [label="(1, 512)"]
	63 [label="(1, 256)"]
	64 [label="(1, 512)"]
	65 [label="(1, 512)"]
	66 [label="(1, 512)"]
	67 [label="(1, 512)"]
	68 [label="(1, 512)"]
	69 [label="(1, 512)"]
	70 [label="(1, 512)"]
	71 [label="(1, 512)"]
	72 [label="(1, 512)"]
	73 [label="(1, 512)"]
	74 [label="(1, 512)"]
	75 [label="(512,)"]
	76 [label="(1,)"]
	0 -> 1 [label="Conv1d(17, 64, 3, 1)"]
	1 -> 2 [label=BatchNormalization1d]
	2 -> 3 [label=BatchNormalization1d]
	3 -> 4 [label=ReLU]
	4 -> 5 [label="Conv1d(64, 64, 3, 1)"]
	4 -> 9 [label=ReLU]
	5 -> 6 [label=BatchNormalization1d]
	6 -> 7 [label=ReLU]
	7 -> 8 [label="Conv1d(64, 64, 3, 1)"]
	8 -> 11 [label=Add]
	9 -> 10 [label="Conv1d(64, 64, 1, 1)"]
	10 -> 11 [label=Add]
	11 -> 12 [label=BatchNormalization1d]
	12 -> 13 [label=ReLU]
	13 -> 14 [label="Conv1d(64, 64, 3, 1)"]
	13 -> 18 [label=ReLU]
	14 -> 15 [label=BatchNormalization1d]
	15 -> 16 [label=ReLU]
	16 -> 17 [label="Conv1d(64, 64, 3, 1)"]
	17 -> 20 [label=Add]
	18 -> 19 [label="Conv1d(64, 64, 1, 1)"]
	19 -> 20 [label=Add]
	20 -> 21 [label=BatchNormalization1d]
	21 -> 22 [label=ReLU]
	22 -> 23 [label="Conv1d(64, 128, 3, 2)"]
	22 -> 27 [label=ReLU]
	23 -> 24 [label=BatchNormalization1d]
	24 -> 25 [label=ReLU]
	25 -> 26 [label="Conv1d(128, 128, 3, 1)"]
	26 -> 29 [label=Add]
	27 -> 28 [label="Conv1d(64, 128, 1, 2)"]
	28 -> 29 [label=Add]
	29 -> 30 [label=BatchNormalization1d]
	30 -> 31 [label=ReLU]
	31 -> 32 [label="Conv1d(128, 128, 3, 1)"]
	31 -> 36 [label=ReLU]
	32 -> 33 [label=BatchNormalization1d]
	33 -> 34 [label=ReLU]
	34 -> 35 [label="Conv1d(128, 128, 3, 1)"]
	35 -> 38 [label=Add]
	36 -> 37 [label="Conv1d(128, 128, 1, 1)"]
	37 -> 38 [label=Add]
	38 -> 39 [label=BatchNormalization1d]
	39 -> 40 [label=ReLU]
	40 -> 41 [label="Conv1d(128, 256, 3, 2)"]
	40 -> 45 [label=ReLU]
	41 -> 42 [label=BatchNormalization1d]
	42 -> 43 [label=ReLU]
	43 -> 44 [label="Conv1d(256, 256, 3, 1)"]
	44 -> 47 [label=Add]
	45 -> 46 [label="Conv1d(128, 256, 1, 2)"]
	46 -> 47 [label=Add]
	47 -> 48 [label=BatchNormalization1d]
	48 -> 49 [label=ReLU]
	49 -> 50 [label="Conv1d(256, 256, 3, 1)"]
	49 -> 54 [label=ReLU]
	50 -> 51 [label=BatchNormalization1d]
	51 -> 52 [label=ReLU]
	52 -> 53 [label="Conv1d(256, 256, 3, 1)"]
	53 -> 56 [label=Add]
	54 -> 55 [label="Conv1d(256, 256, 1, 1)"]
	55 -> 56 [label=Add]
	56 -> 57 [label=BatchNormalization1d]
	57 -> 58 [label=ReLU]
	58 -> 59 [label="Conv1d(256, 512, 3, 2)"]
	58 -> 63 [label=ReLU]
	59 -> 60 [label=BatchNormalization1d]
	60 -> 61 [label=ReLU]
	61 -> 62 [label="Conv1d(512, 512, 3, 1)"]
	62 -> 65 [label=Add]
	63 -> 64 [label="Conv1d(256, 512, 1, 2)"]
	64 -> 65 [label=Add]
	65 -> 66 [label=BatchNormalization1d]
	66 -> 67 [label=ReLU]
	67 -> 68 [label="Conv1d(512, 512, 3, 1)"]
	67 -> 72 [label=ReLU]
	68 -> 69 [label=BatchNormalization1d]
	69 -> 70 [label=ReLU]
	70 -> 71 [label="Conv1d(512, 512, 3, 1)"]
	71 -> 74 [label=Add]
	72 -> 73 [label="Conv1d(512, 512, 1, 1)"]
	73 -> 74 [label=Add]
	74 -> 75 [label=GlobalPooling1d]
	75 -> 76 [label=Dense]
}
